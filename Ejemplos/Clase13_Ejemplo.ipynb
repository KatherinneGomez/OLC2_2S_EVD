{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 13 - Procesamiento de lenguaje natural\n",
        "El Procesamiento de Lenguaje Natural (NLP) es una rama de la inteligencia artificial que permite a las computadoras analizar, entender y generar lenguaje humano.\n",
        "\n",
        "Antes de entrenar modelos, es fundamental preprocesar el texto, ya que el lenguaje natural es ruidoso, ambiguo y no estructurado."
      ],
      "metadata": {
        "id": "FWAF3AIy2Op7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalización de Texto\n",
        "La normalización transforma el texto a una forma uniforme, reduciendo variaciones innecesarias.\n",
        "\n",
        "Incluye:\n",
        "\n",
        "* Convertir a minúsculas\n",
        "* Eliminar signos de puntuación\n",
        "* Eliminar números\n",
        "* Eliminar espacios extra"
      ],
      "metadata": {
        "id": "lgRyfkZ_24fC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UF5Ey1b22M8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a65e5f0-f558-4b12-cca6-c86929a4c26d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hola mundo aquí aprendiendo a normalizar un texto emocionante verdad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "texto = '¡Hola mundo!, aquí aprendiendo a normalizar un texto. Emocionante ¿verdad? 255 '\n",
        "\n",
        "# Convertir a minúsculas\n",
        "texto = texto.lower()\n",
        "#texto\n",
        "\n",
        "# Eliminar números\n",
        "texto = re.sub(r'\\d+', '', texto)\n",
        "#texto\n",
        "\n",
        "# Eliminar puntuación\n",
        "texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "#texto\n",
        "\n",
        "# Eliminar espacios extra\n",
        "texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "texto"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización\n",
        "La tokenización consiste en dividir el texto en unidades más pequeñas llamadas tokens (palabras o símbolos)."
      ],
      "metadata": {
        "id": "w7yCMnug3Dgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "GbyCwvYq3LDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95ca0a0-2b1e-4da7-da2c-d2cef4266496"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenización de palabras\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(texto)\n",
        "tokens"
      ],
      "metadata": {
        "id": "s26qKCi83ROk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382f29a7-8c73-4f9b-ffc4-cc56797f1c5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hola',\n",
              " 'mundo',\n",
              " 'aquí',\n",
              " 'aprendiendo',\n",
              " 'a',\n",
              " 'normalizar',\n",
              " 'un',\n",
              " 'texto',\n",
              " 'emocionante',\n",
              " 'verdad']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librería\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenización de oraciones\n",
        "texto = 'NLP es interesante. Algunas aplicaciones son los chatbots y buscadores.'\n",
        "\n",
        "tokens = sent_tokenize(texto)\n",
        "tokens"
      ],
      "metadata": {
        "id": "_iOQeXQo5FiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f14ea3-9954-4b85-cb82-f5f2b37c6bc8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP es interesante.', 'Algunas aplicaciones son los chatbots y buscadores.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminación de Stopwords\n",
        "\n",
        "Son palabras muy frecuentes que no aportan significado relevante, como:\n",
        "\n",
        "`el, la, los, de, y, en, es…`\n",
        "\n",
        "Eliminar stopwords ayuda a:\n",
        "* reducir ruido\n",
        "* mejorar modelos de clasificación\n",
        "* reducir dimensionalidad"
      ],
      "metadata": {
        "id": "VtqdUfHC5Ygb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "print('Tokens: ', tokens)\n",
        "print('-'*25)\n",
        "tokens_filtrados = [word for word in tokens if word not in stopwords]\n",
        "print(tokens_filtrados)\n"
      ],
      "metadata": {
        "id": "zz4JfJRW5rZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec38ea3-86fb-424d-8ded-2c92f895e559"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['hola', 'mundo', 'aquí', 'aprendiendo', 'a', 'normalizar', 'un', 'texto', 'emocionante', 'verdad']\n",
            "-------------------------\n",
            "['hola', 'mundo', 'aquí', 'aprendiendo', 'normalizar', 'texto', 'emocionante', 'verdad']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "El stemming reduce las palabras a su raíz que no siempre es una palabra real, se utiliza cuando la velocidad es la prioridad.\n",
        "\n",
        "```\n",
        "corriendo → corr\n",
        "\n",
        "jugando → jug\n",
        "```"
      ],
      "metadata": {
        "id": "ccncO-1Q_pwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "palabras = ['corriendo', 'correr', 'corrío', 'corredor', 'correo']\n",
        "\n",
        "[stemmer.stem(p) for p in palabras]"
      ],
      "metadata": {
        "id": "zsNnZmXyAkMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b8eb2e-c26a-4910-884e-ab6586560b16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['corr', 'corr', 'corri', 'corredor', 'corre']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lematización\n",
        "La lematización reduce las palabras a su forma base (lema), usando información lingüística. Es más precisa, pero más costosa computacionalmente.\n",
        "\n",
        "\n",
        "```\n",
        "corriendo → correr\n",
        "\n",
        "mejores → bueno\n",
        "```"
      ],
      "metadata": {
        "id": "bJ5B74zYAv4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de dependencias\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "7cg1GcC3A9WJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d42bd5-a796-4f57-a698-f69aafc5e4a6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la librería\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "doc = nlp('Los niños están corriendo rápidamente en el parque')\n",
        "\n",
        "[(token.text, token.lemma_) for token in doc]"
      ],
      "metadata": {
        "id": "Slkb8iMRA-sU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5dc4333-5627-498d-fc15-865ea4a1a766"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Los', 'el'),\n",
              " ('niños', 'niño'),\n",
              " ('están', 'estar'),\n",
              " ('corriendo', 'correr'),\n",
              " ('rápidamente', 'rápidamente'),\n",
              " ('en', 'en'),\n",
              " ('el', 'el'),\n",
              " ('parque', 'parque')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}